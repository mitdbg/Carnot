#!/usr/bin/env python3
# src/ILCI/indexing/index_chroma.py

# --- SQLite shim --------------------------------------------------------------
__import__("pysqlite3")
import sys as _sys
_sys.modules["sqlite3"] = _sys.modules.pop("pysqlite3")

import os
import re
import signal
import json
import hashlib
import logging
import unicodedata
from typing import List, Dict, Optional, Callable, TypedDict, Literal, Iterable, Union
from dataclasses import dataclass

from tqdm import tqdm
from unidecode import unidecode

import chromadb
from sentence_transformers import SentenceTransformer

# ------------------------------- Types -----------------------------------------

class DocIn(TypedDict):
    doc_id: str
    text: str
    metadata: Optional[Dict[str, object]]  # optional base metadata

EmbeddingFn = Callable[[List[str]], List[List[float]]]  # batch: texts -> vectors

class BuildIndexResult(TypedDict):
    collection: str
    num_docs: int
    num_chunks: int
    dim: int
    metric: Literal["l2", "cosine", "ip"]

# ------------------------------ Logging ----------------------------------------

logger = logging.getLogger(__name__)

# ------------------------------ Helpers ----------------------------------------

def normalize_title_slug(s: str) -> str:
    """Normalize a title to a filesystem- and id-friendly slug."""
    if not s:
        return "untitled"
    t = unicodedata.normalize("NFC", s).strip()
    t = unidecode(t)
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^A-Za-z0-9 _\-.]", "", t).strip().replace(" ", "_")
    return t or "untitled"

def stable_entity_id(title: str, text: str) -> str:
    """Stable id derived from title + text hash."""
    slug = normalize_title_slug(title or "untitled")
    h = hashlib.sha1((title + "\n" + (text or "")).encode("utf-8")).hexdigest()[:8]
    return f"{slug}-{h}"

def read_jsonl(path: str) -> Iterable[Dict]:
    with open(path, "r", encoding="utf-8") as f:
        for idx, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                yield json.loads(line)
            except json.JSONDecodeError as e:
                logger.warning(f"Skipping malformed JSON on line {idx}: {e}")

def build_tokenizer(model_name: str):
    """Use HF AutoTokenizer, fast implementation."""
    from transformers import AutoTokenizer
    return AutoTokenizer.from_pretrained(model_name, use_fast=True)

def chunk_by_tokens(text: str, tokenizer, chunk_tokens: int, overlap_tokens: int) -> List[str]:
    """Token-window chunking (decode each window)."""
    if not text:
        return []
    toks = tokenizer.encode(text, add_special_tokens=False)
    if not toks:
        return []
    chunks: List[str] = []
    step = max(1, chunk_tokens - overlap_tokens)
    for start in range(0, len(toks), step):
        end = min(start + chunk_tokens, len(toks))
        sub = toks[start:end]
        if not sub:
            break
        chunk_text = tokenizer.decode(sub, skip_special_tokens=True).strip()
        if chunk_text:
            chunks.append(chunk_text)
        if end >= len(toks):
            break
    return chunks

# -------------------- Embedding function (collection-level) --------------------

class STEmbeddingFn:
    """
    Collection-level embedding function:
    - wraps SentenceTransformer.encode()
    - normalize_embeddings=True for cosine/IP friendliness
    """
    def __init__(self, model_name: str, device: Optional[str] = None, batch_size: int = 64):
        self.model_name = model_name
        self.model = SentenceTransformer(model_name, device=device)
        self.batch_size = batch_size

    def __call__(self, inputs: List[str]) -> List[List[float]]:
        if not inputs:
            return []
        embs = self.model.encode(
            inputs,
            batch_size=self.batch_size,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False,
        )
        return embs.tolist()

    def name(self) -> str:
        return f"sentence-transformers:{self.model_name}"

# Optional adapter if the user provides an EmbeddingFn instead of a model name.
class CallableEmbeddingAdapter:
    """
    Wraps a plain EmbeddingFn (callable) to look like a Chroma embedding_function.
    """
    def __init__(self, fn: EmbeddingFn, name: str = "user:embedding-fn"):
        self._fn = fn
        self._name = name

    def __call__(self, inputs: List[str]) -> List[List[float]]:
        return self._fn(inputs)

    def name(self) -> str:
        return self._name

# ---------------------------- Chroma upsert helper -----------------------------

def upsert_in_batches(collection, ids: List[str], documents: List[str], metadatas: List[Dict], batch_size: int):
    """
    Call collection.upsert(ids, documents, metadatas) in batches.
    With an embedding_function attached to the collection, embeddings are computed automatically. 
    """
    for i in range(0, len(ids), batch_size):
        j = i + batch_size
        collection.upsert(ids=ids[i:j], documents=documents[i:j], metadatas=metadatas[i:j])

# ------------------------------ Public API -------------------------------------

@dataclass
class BuildParams:
    persist_dir: str = "./chroma_quest"
    collection_name: str = "quest_documents"
    model_name: str = "BAAI/bge-small-en-v1.5"     # used for both tokenizer & SentenceTransformer
    device: Optional[str] = None
    batch_size: int = 256                          # upsert batch size
    chunk_tokens: int = 512
    overlap_tokens: int = 80
    clear_collection: bool = False

def build_index(
    docs_path: str,
    embed_model: Optional[EmbeddingFn],            # you can pass None to use SentenceTransformer
    chunk_size: int,
    overlap_size: int,
    collection: str,
    metric: Literal["l2", "cosine", "ip"] = "cosine",
    *,
    params: Optional[BuildParams] = None,
) -> BuildIndexResult:
    """
    Build an index in Chroma:
      - create/get PersistentClient collection with an embedding_function
      - chunk docs by tokens (HF tokenizer), upsert ids/documents/metadatas in batches
      - embeddings are computed automatically via the collection's embedding_function

    NOTE:
      * We attach an embedding_function to the collection (not passing embeddings to upsert).
      * We use collection.upsert(...) in batches.
    """
    cfg = params or BuildParams(collection_name=collection)
    cfg.collection_name = collection or cfg.collection_name

    logger.info("Starting indexing process...")
    logger.info(f"Collection name: {cfg.collection_name}")
    logger.info(f"Chroma persistence dir: {cfg.persist_dir}")
    logger.info(f"Embedding model (collection-level): {cfg.model_name}")

    os.makedirs(cfg.persist_dir, exist_ok=True)
    client = chromadb.PersistentClient(path=cfg.persist_dir)

    # Choose the collection-level embedding function:
    if embed_model is None:
        embed_fn = STEmbeddingFn(model_name=cfg.model_name, device=cfg.device, batch_size=64)
    else:
        embed_fn = CallableEmbeddingAdapter(embed_model)

    # Optionally clear existing collection
    if cfg.clear_collection:
        try:
            client.delete_collection(cfg.collection_name)
            logger.info(f"Deleted existing collection '{cfg.collection_name}'.")
        except Exception:
            pass

    # Attach embedding_function at collection creation
    collection_obj = client.get_or_create_collection(
        name=cfg.collection_name,
        embedding_function=embed_fn,
    )

    # Build tokenizer
    tokenizer = build_tokenizer(cfg.model_name)
    
    docs_iter = read_jsonl(docs_path)
    total_docs = sum(1 for _ in docs_iter)
    logger.info(f"Found {total_docs} documents to process.")

    p_docs = tqdm(total=total_docs, desc="Documents processed", unit="doc")
    p_chunks = tqdm(total=0, desc="Chunks indexed", unit="chunk")

    batch_ids: List[str] = []
    batch_docs: List[str] = []
    batch_metas: List[Dict] = []
    running_chunk_total = 0
    dim = 0

    interrupted = {"flag": False}
    def handle_sigint(sig, frame):
        interrupted["flag"] = True
        logger.warning("\nInterrupt received. Flushing pending batch before exit...")

    old_handler = signal.signal(signal.SIGINT, handle_sigint)

    try:
        for d in docs_iter:
            if interrupted["flag"]:
                break

            title = (d.get("metadata", {}) or {}).get("title")  # if provided
            if not title:
                title = (d.get("doc_id") or "untitled")
            text = (d.get("text") or "").strip()

            entity_id = stable_entity_id(title, text)
            chunks = chunk_by_tokens(text, tokenizer, chunk_size, overlap_size)
            if not chunks:
                # Fall back to a single, non-empty unit (title)
                chunks = [title]

            n_chunks = len(chunks)
            for idx, chunk in enumerate(chunks):
                cid = f"{entity_id}__{idx:04d}"
                batch_ids.append(cid)
                batch_docs.append(chunk)
                meta = dict(d.get("metadata", {}) or {})
                meta.update({
                    "entity_id": entity_id,
                    "title": title,
                    "chunk_index": idx,
                    "n_chunks": n_chunks,
                    "source": meta.get("source", "inline"),  # caller can override
                    "doc_id": d.get("doc_id", ""),
                })
                batch_metas.append(meta)

            running_chunk_total += n_chunks
            p_docs.update(1)
            p_chunks.total = running_chunk_total
            p_chunks.refresh()

            if len(batch_ids) >= cfg.batch_size:
                upsert_in_batches(collection_obj, batch_ids, batch_docs, batch_metas, cfg.batch_size)
                logger.info(f"Upserted a batch of {len(batch_ids)} chunks. Total indexed: {p_chunks.n + len(batch_ids)}")
                p_chunks.update(len(batch_ids))
                batch_ids.clear(); batch_docs.clear(); batch_metas.clear()

        if batch_ids:
            upsert_in_batches(collection_obj, batch_ids, batch_docs, batch_metas, cfg.batch_size)
            logger.info(f"Upserted final batch of {len(batch_ids)} chunks. Total indexed: {p_chunks.n + len(batch_ids)}")
            p_chunks.update(len(batch_ids))
            batch_ids.clear(); batch_docs.clear(); batch_metas.clear()

    finally:
        signal.signal(signal.SIGINT, old_handler)
        p_docs.close()
        p_chunks.close()

    # We can estimate dim by embedding a tiny probe (or leave 0 if no chunks)
    try:
        probe = ["hello world"]
        # embedding_function is attached to the collection; call it directly:
        vec = embed_fn(probe)[0] if probe else []
        dim = len(vec) if vec else 0
    except Exception:
        dim = 0

    if interrupted["flag"]:
        logger.info("Indexing interrupted after partial flush.")
    else:
        logger.info(f"Done. Successfully indexed {running_chunk_total} chunks from {total_docs} documents.")

    return {
        "collection": cfg.collection_name,
        "num_docs": total_docs,
        "num_chunks": running_chunk_total,
        "dim": dim,
        "metric": metric,
    }
